name: "Desktop/Server"
constraints:
  max_memory_gb: 32
  cpu_cores: 16
  gpu_available: true
  cpu_only: false

models:
  stt: "wav2vec2-base" # Keep base model (fast enough)
  llm: "llama-3.2-1b-instruct" # Smaller, faster model for <800ms target
  tts: "coqui-xtts-v2" # GPU-capable model with voice cloning

runtime:
  target_latency_s: 0.5
  streaming_enabled: true
  predictive_loading: true

audio:
  sample_rate: 22050
  chunk_size: 2048
  channels: 2

vad:
  threshold: 0.75 # Increased from 0.7 to reduce false positives from background noise
  min_speech_duration_s: 0.05 # Reduced from 0.1 to detect speech faster
  max_silence_duration_s: 1.0 # Allow natural speech pauses (1.0s)
  wake_word:
    enabled: false # Disabled for testing, enable for production
    method: "energy" # "energy" (fast, simple) or "openwakeword" (ML-based, accurate)
    # Note: Wake word phrases come from character config (configs/characters/*/wake_words.yaml)

power:
  default_mode: "performance" # performance | balanced | battery_saver
  sleep_after_idle_s: 300
  aggressive_model_unloading: false
  cpu_governor: "performance" # performance | ondemand | powersave

optimizations:
  # Maximum performance
  use_int8_quantization: false
  reduce_sample_rate: false
  batch_size: 4
  memory_mapped_models: true

  # Model-specific optimizations
  llm:
    n_threads: 8 # Balanced thread count
    use_mlock: false # Avoid memory locking issues
    max_batch_size: 4 # Conservative batch size
    n_gpu_layers: 25 # Moderate GPU layers
    use_cpu: false # Use GPU for LLM
    n_ctx: 2048 # Smaller context window for faster processing

  stt:
    use_cpu: true # Keep STT on CPU to avoid conflicts

  tts:
    use_gpu: true # Enable GPU acceleration for TTS
    use_half_precision: false # FP32 for stability (FP16 causes dtype mismatch)
    chunk_size: 64 # Smaller chunks for streaming

# Memory system configuration for desktop/high-end hardware
memory_system:
  enabled: true
  summarization_enabled: true
  smart_caching_enabled: true
